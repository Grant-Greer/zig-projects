#Evolutionary Neural Network Simulation

##Overview

This project implements a subsymbolic multilayer perceptron built using the Zig programming language. 
The neural network is composed of layers of digital neurons, forming a feedforward neural network.
Additionally, a genetic algorithm is employed to simulate the inheritance of advantageous traits, thereby modeling evolutionary dynamics observed in natural systems.

##Contextual Background

The history of subsymbolic systems and neural networks dates back to the 1950s when the field of artificial intelligence (AI) began to take shape.
Researchers sought to create machines capable of simulating human thought processes. Early pioneers like Frank Rosenblatt developed the perceptron, 
a simple neural network model capable of learning from data. This marked the inception of subsymbolic AI, 
which focuses on learning patterns and representations from raw data without relying on symbolic reasoning.

In the decades that followed, advancements in computational power and algorithms led to more sophisticated neural network models. 
The development of multilayer perceptrons (MLPs) introduced the concept of deep learning, where multiple layers of neurons process information in a hierarchical manner. This approach has been fundamental in solving complex tasks such as image recognition and natural language processing.

If you would like to learn more, I suggest reading Melanie Mitchell's book titled "Artificial Intelligence: A Guide for Thinking Humans," which provides a comprehensive overview of the evolution of AI and the challenges it faces. 


##Contributing

Contributions are welcome! If you have suggestions for improvements or new features, feel free to create an issue or submit a pull request.

##License

This project is licensed under the MIT License. See the LICENSE file for more details.


